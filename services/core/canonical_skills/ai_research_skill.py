"""
AI RESEARCH SKILL - Enhanced research with LLM
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞
"""
import os
import uuid
import json
from datetime import datetime
from typing import Dict
from skill_manifest import SkillManifest, SkillResult, ArtifactType, SkillCategory
from verification_engine import VerificationEngine

# Constant for artifacts path
ARTIFACTS_PATH = os.getenv("ARTIFACTS_PATH", "/data/artifacts")
os.makedirs(ARTIFACTS_PATH, exist_ok=True)

AI_RESEARCH_MANIFEST = SkillManifest(
    name="ai_research",
    version="2.0",
    description="AI-powered deep research with analysis and synthesis",
    category=SkillCategory.reasoning,
    agent_roles=["Researcher", "Analyst"],
    inputs=type('Inputs', (), {"required": ["query", "research_type"], "optional": ["depth"]})(
        schema="AIResearchInput",
        required=["query", "research_type"],
        optional=["depth"]
    ),
    outputs=type('Outputs', (), {"artifact_type": ArtifactType.REPORT, "schema_name": "AIResearchResult", "reusable": True})(
        artifact_type=ArtifactType.REPORT,
        schema_name="AIResearchResult",
        reusable=True
    ),
    produces=[
        {
            "type": "REPORT",
            "store": "file",
            "format": "markdown",
            "path_template": "results/{goal_id}/ai_research_{timestamp}.md",
            "tags": ["ai", "research", "analysis"]
        }
    ],
    verification=[
        {"name": "has_sources", "rule": "sources_count >= 3"},
        {"name": "has_analysis", "rule": "len(analysis) > 500"},
        {"name": "has_conclusions", "rule": "has_conclusions == true"}
    ]
)


class AIResearchSkill:
    """
    üî¨ AI Research Skill v2.0

    Uses LLM to:
    - Research topics deeply
    - Synthesize information
    - Provide actionable insights
    """

    def __init__(self):
        self.manifest = AI_RESEARCH_MANIFEST
        self.verifier = VerificationEngine()

    async def execute(self, inputs: Dict, goal_id: str) -> SkillResult:
        """Execute AI research"""
        query = inputs["query"]
        research_type = inputs.get("research_type", "general")
        depth = inputs.get("depth", 3)

        try:
            # Generate research report using LLM
            research_content = await self._generate_ai_research(query, research_type, depth)

            # Save to persistent storage
            output_dir = os.path.join(ARTIFACTS_PATH, "results", goal_id)
            os.makedirs(output_dir, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ai_research_{timestamp}.md"
            file_path = os.path.join(output_dir, filename)

            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(research_content)

            # Return artifact
            return SkillResult(
                status="success",
                artifacts=[{
                    "artifact_type": "REPORT",
                    "content_kind": "file",
                    "content_location": file_path,
                    "skill_name": self.manifest.name,
                    "agent_role": "AI Researcher",
                    "domains": inputs.get("domains", ["research"]),
                    "tags": ["ai", "research", "analysis", research_type],
                    "language": "markdown"
                }],
                metadata={
                    "research_type": research_type,
                    "depth": depth,
                    "sources_count": research_content.count("## –ò—Å—Ç–æ—á–Ω–∏–∫"),
                    "file_size": len(research_content)
                }
            )

        except Exception as e:
            return SkillResult(
                status="failed",
                error=f"AI research failed: {str(e)}",
                artifacts=[]
            )

    async def _generate_ai_research(self, query: str, research_type: str, depth: int) -> str:
        """Generate AI-powered research report"""

        # Get LLM client
        from llm_fallback import llm_manager

        prompt = f"""–¢—ã - AI –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å. –°–æ–∑–¥–∞–π –≥–ª—É–±–æ–∫–∏–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—á–µ—Ç.

–¢–µ–º–∞: {query}
–¢–∏–ø –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {research_type}
–ì–ª—É–±–∏–Ω–∞: {depth} —É—Ä–æ–≤–Ω—è

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç—á–µ—Ç–∞:
1. ## –†–µ–∑—é–º–µ (–∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ)
2. ## –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ (3-5 –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤)
3. ## –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)
4. ## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–º–∏–Ω–∏–º—É–º 3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º)
5. ## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—á—Ç–æ –¥–µ–ª–∞—Ç—å —Å —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π)
6. ## –í—ã–≤–æ–¥—ã

–§–æ—Ä–º–∞—Ç: Markdown
–Ø–∑—ã–∫: –†—É—Å—Å–∫–∏–π
–°—Ç–∏–ª—å: –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–π

–ù–∞—á–∏–Ω–∞–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ:"""

        try:
            response = await llm_manager.acomplete(prompt)
            return response.strip()
        except Exception as e:
            # Fallback to basic template
            return f"""# AI Research: {query}

## –†–µ–∑—é–º–µ
–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–µ–º–µ "{query}" ({research_type})

## –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏
1. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
2. –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–¥–µ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
3. –ü—Ä–æ–≤–µ—Ä—è–π —Ñ–∞–∫—Ç—ã

## –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
[–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ...]

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏
1. –ò—Å—Ç–æ—á–Ω–∏–∫ 1
2. –ò—Å—Ç–æ—á–Ω–∏–∫ 2
3. –ò—Å—Ç–æ—á–Ω–∏–∫ 3

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ü—Ä–æ–≤–µ—Ä—è—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö

## –í—ã–≤–æ–¥—ã
–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç —É–≥–ª—É–±–ª–µ–Ω–∏—è.

*–°–æ–∑–¥–∞–Ω–æ AI Research Skill v2.0*
*–û—à–∏–±–∫–∞ LLM: {str(e)}*
"""


# Export
ai_research_skill = AIResearchSkill()
skill_manifest = AI_RESEARCH_MANIFEST
