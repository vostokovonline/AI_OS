"""
LangChain LLM with Fallback Support
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è llm_fallback —Å LangChain ChatOpenAI
"""
from typing import Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult, LLMResult
import os


class ChatOpenAIWithFallback(ChatOpenAI):
    """
    ChatOpenAI —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Groq fallback.

    –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤—ã–∑–æ–≤—ã –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç llm_fallback –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
    404 –æ—à–∏–±–æ–∫ –æ—Ç Groq –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –Ω–∞ Ollama.
    """

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å fallback –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π.

        LangChain –≤—ã–∑—ã–≤–∞–µ—Ç —ç—Ç–æ—Ç –º–µ—Ç–æ–¥. –ú—ã –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º
        llm_fallback –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –Ω–∞ Ollama –ø—Ä–∏ 404 –æ—Ç Groq.
        """
        # Import here to avoid circular import
        import sys
        sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

        try:
            from llm_fallback import chat_with_fallback_sync
            from langchain_core.messages import message_to_dict

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º LangChain messages –≤ —Ñ–æ—Ä–º–∞—Ç OpenAI
            openai_messages = []
            for msg in messages:
                if hasattr(msg, 'type'):
                    openai_messages.append({
                        "role": msg.type,
                        "content": msg.content
                    })
                else:
                    # Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ messages
                    openai_messages.append({
                        "role": "user",
                        "content": str(msg.content)
                    })

            # –í—ã–∑—ã–≤–∞–µ–º LLM —Å fallback
            model = self.model_name or os.getenv("LLM_MODEL", "groq/llama-3.3-70b-versatile")

            result = chat_with_fallback_sync(
                model=model,
                messages=openai_messages,
                temperature=self.temperature,
                **kwargs
            )

            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ LangChain —Ñ–æ—Ä–º–∞—Ç
            from langchain_core.outputs import ChatGeneration, ChatResult
            from langchain_core.messages import AIMessage

            content = result["choices"][0]["message"]["content"]

            generation = ChatGeneration(
                message=AIMessage(content=content)
            )

            return ChatResult(generations=[generation])

        except ImportError:
            # –ï—Å–ª–∏ llm_fallback –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
            logger.info("‚ö†Ô∏è llm_fallback not available, using default ChatOpenAI")
            return super()._generate(messages, stop, run_manager, **kwargs)


def get_model_with_fallback(role="DEFAULT"):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç LangChain –º–æ–¥–µ–ª—å —Å fallback –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏ —É—á–µ—Ç–æ–º —Ä–æ–ª–∏.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á.
    """
    # Mapping —Ä–æ–ª–µ–π –Ω–∞ –º–æ–¥–µ–ª–∏
    MODEL_MAPPING = {
        "SUPERVISOR": "ollama/gpt-oss:120b-cloud",         # ‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–æ—É—Ç–∏–Ω–≥ (120B)
        "CODER": "ollama/qwen3-coder:480b-cloud",          # üíª –ö–æ–¥ (480B)
        "PM": "ollama/gpt-oss:120b-cloud",                 # üéØ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (120B)
        "RESEARCHER": "ollama/qwen3-coder:480b-cloud",     # üîç –ü–æ–∏—Å–∫ (480B)
        "INTELLIGENCE": "ollama/deepseek-v3.1:671b-cloud", # üß† –°–ª–æ–∂–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (671B)
        "DEFAULT": "ollama/qwen3-coder:480b-cloud"
    }

    model_name = MODEL_MAPPING.get(role, os.getenv("LLM_MODEL", "ollama/qwen3-coder:480b-cloud"))

    # Temperature –ø–æ —Ä–æ–ª–∏
    if role == "SUPERVISOR":
        temp = 0.1
    elif role == "INTELLIGENCE":
        temp = 0.3
    else:
        temp = 0.2

    return ChatOpenAIWithFallback(
        base_url=os.getenv("LLM_BASE_URL"),
        api_key=os.getenv("OPENAI_API_KEY", "sk-1234"),
        model=model_name,
        temperature=temp,
        request_timeout=120  # 2 –º–∏–Ω—É—Ç—ã (–≤—Å–µ –º–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä—ã–µ)
    )
