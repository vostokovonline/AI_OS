"""
SEMANTIC MEMORY - v3.0
–ò–∑–ª–µ—á–µ–Ω–∏–µ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
Memory ‚â† Logs - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏, –∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è

v3.1: Added Milvus vector search integration
"""
import uuid
import os
import json
import httpx
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from langchain_core.messages import HumanMessage
from sqlalchemy import select, func, delete
from database import AsyncSessionLocal
from models import Goal, Thought
from agent_graph import app_graph

MEMORY_URL = os.getenv("MEMORY_URL", "http://memory:8001")


class SemanticMemory:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å - —Ö—Ä–∞–Ω–∏—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

    Types of patterns:
    - success_patterns: –ß—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ
    - failure_patterns: –ß—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –∏ –ø–æ—á–µ–º—É
    - decomposition_patterns: –ö–∞–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã
    - agent_effectiveness: –ö–∞–∫–∏–µ agent + model –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç
    - domain_patterns: –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –¥–æ–º–µ–Ω–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

    –û—Ç–ª–∏—á–∏–µ –æ—Ç logs:
    - Logs: "–ó–∞–ø—É—Å—Ç–∏–ª–∏ agent X –≤ 12:00"
    - Memory: "Agent X —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è domain Y –ø—Ä–∏ —É—Å–ª–æ–≤–∏—è—Ö Z"
    """

    async def store_pattern(
        self,
        pattern_type: str,
        content: Dict,
        source_goal_id: str,
        confidence: float = 0.5
    ) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ø–∞–º—è—Ç—å

        Args:
            pattern_type: –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            source_goal_id: ID —Ü–µ–ª–∏ –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –∏–∑–≤–ª–µ—á–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω
            confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å 0.0-1.0

        Returns:
            ID —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        """
        from models import Thought

        # üÜï DEDUPLICATION: Check for similar existing patterns
        existing = await self._find_similar_pattern(pattern_type, content)
        if existing:
            # Update confidence instead of creating duplicate
            return await self._update_pattern_confidence(existing, confidence)

        async with AsyncSessionLocal() as db:
            thought = Thought(
                content=f"{pattern_type}: {content}",
                source=source_goal_id,
                status="active" if confidence > 0.5 else "tentative"
            )
            db.add(thought)
            await db.commit()
            await db.refresh(thought)
            
            pattern_id = str(thought.id)
            
            # üÜï Also store in Milvus for vector search
            content["confidence"] = confidence
            await self.store_pattern_vector(pattern_type, content, pattern_id)

            return pattern_id

    async def _find_similar_pattern(
        self,
        pattern_type: str,
        content: Dict,
        similarity_threshold: float = 0.8
    ) -> Optional[str]:
        """
        –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –≤ –ø–∞–º—è—Ç–∏.
        
        Args:
            pattern_type: –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0)
            
        Returns:
            ID –ø–æ—Ö–æ–∂–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∏–ª–∏ None
        """
        from models import Thought
        
        # Use vector search for similarity
        text_repr = self._pattern_to_text(pattern_type, content)
        similar = await self.retrieve_similar_patterns_vector(text_repr, limit=3)
        
        for pattern in similar:
            # Check type match
            if pattern.get("pattern_type") != pattern_type:
                continue
            
            # Check key fields similarity
            if self._calculate_similarity(content, pattern) >= similarity_threshold:
                return pattern.get("id")
        
        return None

    def _calculate_similarity(self, content1: Dict, content2: Dict) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        
        Simple Jaccard similarity on key fields.
        """
        key_fields = ["goal_type", "domains", "success_factors", "mistakes"]
        
        matches = 0
        total = 0
        
        for field in key_fields:
            if field in content1 or field in content2:
                total += 1
                val1 = set(content1.get(field, []) or [])
                val2 = set(content2.get(field, []) or [])
                
                if val1 and val2:
                    intersection = len(val1 & val2)
                    union = len(val1 | val2)
                    matches += intersection / union if union > 0 else 0
                elif val1 == val2:
                    matches += 1
        
        return matches / total if total > 0 else 0.0

    async def _update_pattern_confidence(
        self,
        pattern_id: str,
        new_confidence: float
    ) -> str:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç confidence —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞.
        
        Args:
            pattern_id: ID –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            new_confidence: –ù–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ confidence
            
        Returns:
            ID –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        """
        from models import Thought
        
        async with AsyncSessionLocal() as db:
            stmt = select(Thought).where(Thought.id == uuid.UUID(pattern_id))
            result = await db.execute(stmt)
            thought = result.scalar_one_or_none()
            
            if thought:
                # Boost confidence if new evidence supports it
                old_confidence = thought.content.get("confidence", 0.5) if isinstance(thought.content, dict) else 0.5
                boosted = min(1.0, (old_confidence + new_confidence) / 2 + 0.1)
                
                # Update status if confidence crosses threshold
                if boosted > 0.5:
                    thought.status = "active"
                
                await db.commit()
                print(f"üîÑ Updated pattern {pattern_id}: confidence {old_confidence:.2f} ‚Üí {boosted:.2f}")
            
            return pattern_id

    async def extract_success_pattern(self, goal_id: str, reflection: Dict) -> Dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω —É—Å–ø–µ—Ö–∞ –∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ü–µ–ª–∏

        Args:
            goal_id: ID –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ü–µ–ª–∏
            reflection: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –æ—Ç GoalReflector

        Returns:
            –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        """
        async with AsyncSessionLocal() as db:
            stmt = select(Goal).where(Goal.id == uuid.UUID(goal_id))
            result = await db.execute(stmt)
            goal = result.scalar_one_or_none()

            if not goal:
                return {"error": "Goal not found"}

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —É—Å–ø–µ—Ö–∞
        success_pattern = {
            "pattern_type": "success",
            "goal_type": goal.goal_type,
            "depth_level": goal.depth_level,
            "domains": goal.domains,
            "success_factors": reflection.get("success_factors", []),
            "lessons_learned": reflection.get("lessons_learned", []),
            "patterns": reflection.get("patterns", []),
            "extracted_at": datetime.now().isoformat(),
            "source_goal_id": goal_id
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
        pattern_id = await self.store_pattern(
            "success_pattern",
            success_pattern,
            goal_id,
            confidence=0.8
        )

        return {
            "pattern_id": pattern_id,
            "pattern": success_pattern
        }

    async def extract_failure_pattern(self, goal_id: str, reflection: Dict) -> Dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ—É–¥–∞—á–∏ –∏–∑ –ø—Ä–æ–≤–∞–ª—å–Ω–æ–π —Ü–µ–ª–∏

        Args:
            goal_id: ID —Ü–µ–ª–∏
            reflection: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏

        Returns:
            –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        """
        async with AsyncSessionLocal() as db:
            stmt = select(Goal).where(Goal.id == uuid.UUID(goal_id))
            result = await db.execute(stmt)
            goal = result.scalar_one_or_none()

            if not goal:
                return {"error": "Goal not found"}

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ—É–¥–∞—á–∏
        failure_pattern = {
            "pattern_type": "failure",
            "goal_type": goal.goal_type,
            "depth_level": goal.depth_level,
            "domains": goal.domains,
            "root_causes": reflection.get("root_causes", []),
            "mistakes": reflection.get("mistakes", []),
            "missing_resources": reflection.get("missing_resources", []),
            "extracted_at": datetime.now().isoformat(),
            "source_goal_id": goal_id
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
        pattern_id = await self.store_pattern(
            "failure_pattern",
            failure_pattern,
            goal_id,
            confidence=0.7
        )

        return {
            "pattern_id": pattern_id,
            "pattern": failure_pattern
        }

    async def extract_decomposition_pattern(
        self,
        parent_goal: Goal,
        subgoals: List[Goal]
    ) -> Dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏

        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç:
        - –°–∫–æ–ª—å–∫–æ –ø–æ–¥—Ü–µ–ª–µ–π —Å–æ–∑–¥–∞–Ω–æ
        - –ö–∞–∫–∏–µ —Ç–∏–ø—ã –ø–æ–¥—Ü–µ–ª–µ–π
        - –ö–∞–∫–∏–µ –¥–æ–º–µ–Ω—ã –ø–æ–∫—Ä—ã—Ç—ã
        - –ù–∞—Å–∫–æ–ª—å–∫–æ —ç—Ç–æ –æ–∫–∞–∑–∞–ª–æ—Å—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º
        """
        decomposition_pattern = {
            "pattern_type": "decomposition",
            "parent_goal_type": parent_goal.goal_type,
            "parent_depth": parent_goal.depth_level,
            "parent_domains": parent_goal.domains,
            "subgoals_count": len(subgoals),
            "subgoals_types": [sg.goal_type for sg in subgoals],
            "subgoals_domains": list(set([d for sg in subgoals for d in (sg.domains or [])])),
            "depth_distribution": [sg.depth_level for sg in subgoals],
            "extracted_at": datetime.now().isoformat()
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        pattern_id = await self.store_pattern(
            "decomposition_pattern",
            decomposition_pattern,
            str(parent_goal.id),
            confidence=0.6
        )

        return {
            "pattern_id": pattern_id,
            "pattern": decomposition_pattern
        }

    async def track_agent_effectiveness(
        self,
        agent_role: str,
        model_name: str,
        task_type: str,
        success: bool,
        duration_ms: float,
        context: Dict
    ) -> str:
        """
        –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞ + –º–æ–¥–µ–ª–∏

        Examples:
        - "Coder + gpt-4 —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è refactoring"
        - "Researcher + claude-opus –ª—É—á—à–µ –¥–ª—è analysis"
        """
        effectiveness_pattern = {
            "pattern_type": "agent_effectiveness",
            "agent_role": agent_role,
            "model_name": model_name,
            "task_type": task_type,
            "success": success,
            "duration_ms": duration_ms,
            "context": context,  # domains, goal_type, etc.
            "extracted_at": datetime.now().isoformat()
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        pattern_id = await self.store_pattern(
            "agent_effectiveness",
            effectiveness_pattern,
            context.get("goal_id", ""),
            confidence=0.9 if success else 0.4
        )

        return pattern_id

    async def retrieve_similar_patterns(
        self,
        pattern_type: str,
        goal_type: str = None,
        domains: List[str] = None,
        limit: int = 5
    ) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –ø–∞–º—è—Ç–∏

        Args:
            pattern_type: –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            goal_type: –¢–∏–ø —Ü–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            domains: –î–æ–º–µ–Ω—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit: –ú–∞–∫—Å–∏–º—É–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        from models import Thought

        async with AsyncSessionLocal() as db:
            # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            stmt = select(Thought).where(
                Thought.content.like(f"{pattern_type}%")
            )

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å—Ç–∞—Ç—É—Å—É
            stmt = stmt.where(Thought.status == "active")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            stmt = stmt.order_by(Thought.created_at.desc())

            # –õ–∏–º–∏—Ç
            stmt = stmt.limit(limit * 2)  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ, –ø–æ—Ç–æ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º

            result = await db.execute(stmt)
            thoughts = result.scalars().all()

            # –ü–∞—Ä—Å–∏–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            patterns = []
            for thought in thoughts:
                try:
                    import json
                    # content = "success_pattern: {...}"
                    content = thought.content.split(": ", 1)[1]
                    pattern = json.loads(content)

                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ goal_type –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if goal_type and pattern.get("goal_type") != goal_type:
                        continue

                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ domains –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if domains:
                        pattern_domains = pattern.get("domains", [])
                        if not any(d in pattern_domains for d in domains):
                            continue

                    patterns.append({
                        "id": str(thought.id),
                        "pattern": pattern,
                        "created_at": thought.created_at.isoformat()
                    })

                    if len(patterns) >= limit:
                        break

                except (ValueError, KeyError, json.JSONDecodeError) as e:
                    logger.debug("pattern_parse_error", thought_id=str(thought.id), error=str(e))
                    continue
                except Exception as e:
                    logger.warning("unexpected_pattern_error", thought_id=str(thought.id), error=str(e))
                    continue

            return patterns

    async def get_recommendations(
        self,
        goal: Goal,
        task_type: str = None
    ) -> Dict:
        """
        –î–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

        Args:
            goal: –¶–µ–ª—å –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π –¥–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏

        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        success_patterns = await self.retrieve_similar_patterns(
            "success_pattern",
            goal_type=goal.goal_type,
            domains=goal.domains,
            limit=3
        )

        failure_patterns = await self.retrieve_similar_patterns(
            "failure_pattern",
            goal_type=goal.goal_type,
            domains=goal.domains,
            limit=3
        )

        agent_patterns = await self.retrieve_similar_patterns(
            "agent_effectiveness",
            limit=5
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = {
            "success_factors": list(set([
                factor
                for p in success_patterns
                for factor in p["pattern"].get("success_factors", [])
            ])),
            "pitfalls": list(set([
                pitfall
                for p in failure_patterns
                for pitfall in p["pattern"].get("mistakes", [])
            ])),
            "effective_agents": [
                {
                    "agent": p["pattern"]["agent_role"],
                    "model": p["pattern"]["model_name"],
                    "success_rate": "high" if p["pattern"]["success"] else "low"
                }
                for p in agent_patterns
                if p["pattern"]["success"]
            ]
        }

        return recommendations

    async def store_pattern_vector(
        self,
        pattern_type: str,
        content: Dict,
        pattern_id: str
    ) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –≤ Milvus –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
        
        Args:
            pattern_type: –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            pattern_id: ID –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∏–∑ PostgreSQL
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –∏–Ω–∞—á–µ
        """
        try:
            # –°–æ–∑–¥–∞—ë–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è embedding
            text_repr = self._pattern_to_text(pattern_type, content)
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{MEMORY_URL}/remember",
                    json={
                        "text": text_repr,
                        "type": "semantic",
                        "metadata": {
                            "pattern_id": pattern_id,
                            "pattern_type": pattern_type,
                            "goal_type": content.get("goal_type"),
                            "domains": content.get("domains", []),
                            "confidence": content.get("confidence", 0.5)
                        }
                    }
                )
                return response.status_code == 200
        except Exception as e:
            logger.info(f"‚ö†Ô∏è Milvus store error: {e}")
            return False

    async def retrieve_similar_patterns_vector(
        self,
        query_text: str,
        limit: int = 5
    ) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ Milvus –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É.
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
            limit: –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{MEMORY_URL}/search",
                    json={
                        "text": query_text,
                        "type": "semantic",
                        "top_k": limit
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    matches = data.get("matches", [])
                    
                    results = []
                    for match in matches:
                        try:
                            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                            if isinstance(match, str) and match.startswith("{"):
                                pattern = json.loads(match)
                                results.append(pattern)
                        except json.JSONDecodeError as e:
                            logger.debug("milvus_match_json_error", match=match[:100], error=str(e))
                            continue
                        except Exception as e:
                            logger.warning("milvus_match_error", match=match[:100], error=str(e))
                            continue
                    
                    return results
        except Exception as e:
            logger.info(f"‚ö†Ô∏è Milvus search error: {e}")
        
        return []

    async def cleanup_old_patterns(self, days: int = 30) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å low confidence.
        
        Args:
            days: –£–¥–∞–ª–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        async with AsyncSessionLocal() as db:
            cutoff = datetime.now() - timedelta(days=days)
            
            # –£–¥–∞–ª—è–µ–º tentative –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç–∞—Ä—à–µ cutoff
            stmt = delete(Thought).where(
                Thought.status == "tentative",
                Thought.created_at < cutoff
            )
            
            result = await db.execute(stmt)
            await db.commit()
            
            deleted_count = result.rowcount
            logger.info(f"üßπ Cleaned up {deleted_count} old patterns")
            
            return deleted_count

    def _pattern_to_text(self, pattern_type: str, content: Dict) -> str:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –≤ —Ç–µ–∫—Å—Ç –¥–ª—è embedding.
        
        Args:
            pattern_type: –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        """
        parts = [f"Pattern type: {pattern_type}"]
        
        if "goal_type" in content:
            parts.append(f"Goal type: {content['goal_type']}")
        
        if "domains" in content:
            parts.append(f"Domains: {', '.join(content['domains'])}")
        
        if "success_factors" in content:
            parts.append(f"Success factors: {', '.join(content['success_factors'])}")
        
        if "lessons_learned" in content:
            parts.append(f"Lessons: {', '.join(content['lessons_learned'])}")
        
        if "root_causes" in content:
            parts.append(f"Root causes: {', '.join(content['root_causes'])}")
        
        if "mistakes" in content:
            parts.append(f"Mistakes: {', '.join(content['mistakes'])}")
        
        return " | ".join(parts)

    async def store_pattern_graph(
        self,
        pattern_type: str,
        content: Dict,
        pattern_id: str
    ) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤—è–∑–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –≤ Neo4j.
        
        –°–æ–∑–¥–∞—ë—Ç —É–∑–ª—ã –¥–ª—è:
        - Pattern
        - Domains
        - Goal types
        
        –ò —Å–≤—è–∑–∏ –º–µ–∂–¥—É –Ω–∏–º–∏.
        
        Args:
            pattern_type: –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            pattern_id: ID –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                # Create pattern node
                await client.post(
                    f"{MEMORY_URL}/add_fact",
                    json={
                        "subject": f"Pattern:{pattern_id}",
                        "predicate": "TYPE",
                        "object": pattern_type
                    }
                )
                
                # Create domain relationships
                for domain in content.get("domains", []):
                    await client.post(
                        f"{MEMORY_URL}/add_fact",
                        json={
                            "subject": f"Pattern:{pattern_id}",
                            "predicate": "RELATES_TO_DOMAIN",
                            "object": domain
                        }
                    )
                
                # Create goal type relationship
                if "goal_type" in content:
                    await client.post(
                        f"{MEMORY_URL}/add_fact",
                        json={
                            "subject": f"Pattern:{pattern_id}",
                            "predicate": "APPLIES_TO_GOAL_TYPE",
                            "object": content["goal_type"]
                        }
                    )
                
                return True
                
        except Exception as e:
            print(f"‚ö†Ô∏è Neo4j store error: {e}")
            return False

    async def batch_store_patterns_vector(
        self,
        patterns: List[Dict]
    ) -> int:
        """
        Batch —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ Milvus.
        
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –æ–¥–∏–Ω HTTP –∑–∞–ø—Ä–æ—Å –≤–º–µ—Å—Ç–æ N.
        
        Args:
            patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö
        """
        # Milvus doesn't support batch insert via HTTP API
        # Use sequential for now, but in parallel
        import asyncio
        
        tasks = []
        for p in patterns:
            task = self.store_pattern_vector(
                p["pattern_type"],
                p["content"],
                p["pattern_id"]
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = sum(1 for r in results if r is True)
        return success_count

    async def get_stats(self) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
        
        Returns:
            Dict —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø–∞–º—è—Ç–∏
        """
        stats = {
            "postgresql": {},
            "milvus": {},
            "neo4j": {},
            "redis": {}
        }
        
        # PostgreSQL stats
        try:
            async with AsyncSessionLocal() as db:
                # Total patterns
                result = await db.execute(select(func.count(Thought.id)))
                stats["postgresql"]["total_patterns"] = result.scalar() or 0
                
                # By status
                result = await db.execute(
                    select(Thought.status, func.count(Thought.id))
                    .group_by(Thought.status)
                )
                stats["postgresql"]["by_status"] = {
                    row[0]: row[1] for row in result.all()
                }
                
                # By pattern type (parse from content)
                result = await db.execute(
                    select(Thought.content)
                    .where(Thought.content.like("%_pattern:%"))
                    .limit(100)
                )
                pattern_types = {}
                for row in result.scalars().all():
                    try:
                        content = row if isinstance(row, str) else str(row)
                        ptype = content.split(":")[0] if ":" in content else "unknown"
                        pattern_types[ptype] = pattern_types.get(ptype, 0) + 1
                    except:
                        pass
                stats["postgresql"]["by_pattern_type"] = pattern_types
                
        except Exception as e:
            stats["postgresql"]["error"] = str(e)
        
        # Milvus stats
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                # Check if memory service is responding
                response = await client.get(f"{MEMORY_URL}/user/analysis")
                stats["milvus"]["status"] = "connected"
        except Exception as e:
            stats["milvus"]["status"] = "disconnected"
            stats["milvus"]["error"] = str(e)[:100]
        
        # Redis stats (MemorySignal)
        try:
            from memory_signal import persistent_memory_registry
            stats["redis"]["memory_signals"] = persistent_memory_registry.summary()
        except Exception as e:
            stats["redis"]["error"] = str(e)
        
        return stats


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
semantic_memory = SemanticMemory()
